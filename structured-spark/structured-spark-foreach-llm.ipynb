{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8718ffc9-c782-4120-b75c-27ec0ba94442",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import HiveContext, SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from delta import *\n",
    "from chunkipy import TextChunker, TokenEstimator\n",
    "from numpy import exp\n",
    "import boto3\n",
    "import builtins\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23e4497b-1296-433b-b0f4-9925ef918c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-17 17:42:24.192976: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-17 17:42:24.193030: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-17 17:42:24.194439: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-17 17:42:24.202284: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-17 17:42:25.014089: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BertTokenizer, pipeline\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", framework=\"pt\", device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a18eb102-bdcc-4957-bea5-9a3b7673188e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark-3.5.0-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ubuntu/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ubuntu/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-a48b2c38-b490-4cbd-ba8e-58acddd3c61d;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.12;3.1.0 in local-m2-cache\n",
      "\tfound io.delta#delta-storage;3.1.0 in local-m2-cache\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in local-m2-cache\n",
      ":: resolution report :: resolve 136ms :: artifacts dl 7ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.12;3.1.0 from local-m2-cache in [default]\n",
      "\tio.delta#delta-storage;3.1.0 from local-m2-cache in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from local-m2-cache in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-a48b2c38-b490-4cbd-ba8e-58acddd3c61d\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/4ms)\n",
      "24/03/17 17:42:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "builder = SparkSession.builder.appName(\"amzn-reviews\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.driver.cores\", \"2\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\",\"28g\") \\\n",
    "    .config(\"spark.executor.cores\",\"5\") \\\n",
    "    .config(\"spark.executor.instances\",\"2\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\",True) \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", True) \\\n",
    "    .config(\"spark.sql.parquet.mergeSchema\", False) \\\n",
    "    .config(\"spark.hadoop.parquet.enable.summary-metadata\", False) \\\n",
    "    .enableHiveSupport()\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0aa633f6-537f-4c20-9e65-3f55ec4385d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Variables\n",
    "BOOTSTRAP_SERVERS = \"boot-uuoa1swb.c1.kafka-serverless.us-east-1.amazonaws.com:9098\"\n",
    "REGION = \"us-east-1\"\n",
    "TOPIC = \"amznbookreviews\"\n",
    "SINK_BUCKET = \"amzn-customer-reviews-228924278364\"\n",
    "SINK_PREFIX = \"sink/llm-transformed-stream-foreachbatch/\"\n",
    "SINK_PATH = f\"s3a://{SINK_BUCKET}/{SINK_PREFIX}\"\n",
    "CHECKPOINT_PATH = \"s3a://amzn-customer-reviews-228924278364/checkpoint/\"\n",
    "MAX_ROW_SINK = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56d8c9d5-4a4c-42b9-a57c-22792f16785c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get secrets credential for S3a\n",
    "REGION = \"us-east-1\"\n",
    "client = boto3.client('secretsmanager',region_name=REGION)\n",
    "response = client.get_secret_value(\n",
    "    SecretId='s3all'\n",
    ")\n",
    "accessJson = json.loads(response['SecretString'])\n",
    "accessKeyId = accessJson['accessKey']\n",
    "secretAccessKey = accessJson['secretAccess']\n",
    "\n",
    "# Configure S3a\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", accessKeyId)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", secretAccessKey)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.us-east-1.amazonaws.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "029f6121-9171-4e3c-81bc-694f4b92402b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertTokenEstimator(TokenEstimator):\n",
    "    def __init__(self):\n",
    "        self.bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    def estimate_tokens(self, text):\n",
    "        return len(self.bert_tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b02f69a0-de54-4428-a8fe-e8e96c1a6187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPdfLLM(df):\n",
    "    # Variables\n",
    "    # Keep max token input much lower than max of 1024\n",
    "    max_token_input = 900\n",
    "    min_token_output = 130\n",
    "    chunk_size = 512\n",
    "    min_token_size = 200\n",
    "    \n",
    "    # Initializing summary to return\n",
    "    summary_text = \"\"\n",
    "    \n",
    "    # Initialize BertEstimator\n",
    "    bert_token_estimator = BertTokenEstimator()\n",
    "\n",
    "    # Initialize Classifier\n",
    "    classifier = pipeline(\n",
    "        task=\"zero-shot-classification\",\n",
    "        device=0,\n",
    "        model=\"facebook/bart-large-mnli\"\n",
    "    )\n",
    "\n",
    "    classifier_labels = ['negative', 'positive', 'neutral']\n",
    "\n",
    "    def batchSize(token_count, max_token_input):\n",
    "        quotient = token_count / max_token_input\n",
    "        remainder = token_count % max_token_input\n",
    "        return math.floor(quotient), remainder\n",
    "\n",
    "    def chunk(txt):\n",
    "        token_count = bert_token_estimator.estimate_tokens(txt)\n",
    "        text_chunker = TextChunker(chunk_size, tokens=True, token_estimator=BertTokenEstimator())\n",
    "        chunks = text_chunker.chunk(txt)\n",
    "\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            yield chunk\n",
    "\n",
    "    def sentiment_func(result):\n",
    "#        result = classifier(txt,classifier_labels,multi_label=True)\n",
    "        labels = result[\"labels\"]\n",
    "        score = result[\"scores\"]\n",
    "        result_dict = {labels[i]: score[i] for i in range(len(labels))}\n",
    "\n",
    "        # builtins required so as not to confuse with pyspark max() function\n",
    "        sentiment = builtins.max(result_dict, key=result_dict.get)\n",
    "\n",
    "        return sentiment\n",
    "\n",
    "    def summarizer_func(corpus):\n",
    "        summary_response = summarizer(corpus, max_length=130, min_length=30, do_sample=False)\n",
    "        summary = summary_response[0][\"summary_text\"]\n",
    "        return summary\n",
    "        \n",
    "    # Main code\n",
    "    bartUpdated_list = []\n",
    "    sentimentAnalyzed_list = []\n",
    "    sentiment_list = []\n",
    "    bartSummary_list = []\n",
    "    reviewText = [d.reviewText for idx, d in df.iterrows()]\n",
    "    \n",
    "    for r in reviewText:\n",
    " \n",
    "        token_count = bert_token_estimator.estimate_tokens(r)\n",
    "\n",
    "        if token_count > max_token_input:\n",
    "\n",
    "            # Sentiment chunking\n",
    "\n",
    "            text_chunker = TextChunker(chunk_size, tokens=True, token_estimator=BertTokenEstimator())\n",
    "            chunks = text_chunker.chunk(r)\n",
    "\n",
    "            sentiment_results_list = []\n",
    "\n",
    "            sum_positive = 0\n",
    "            sum_negative = 0\n",
    "            sum_neutral = 0\n",
    "\n",
    "            corpus_chunks = []\n",
    "            \n",
    "            for i, chunk in enumerate(chunks):\n",
    "\n",
    "                # Sentiment\n",
    "                result = classifier(chunk,classifier_labels,multi_label=True)\n",
    "                each_label = result[\"labels\"]\n",
    "                each_score = result[\"scores\"]\n",
    "\n",
    "                labels_positive_index = each_label.index(\"positive\")\n",
    "                labels_negative_index = each_label.index(\"negative\")\n",
    "                labels_neutral_index = each_label.index(\"neutral\")\n",
    "\n",
    "                sum_negative = sum_negative + result[\"scores\"][labels_negative_index]\n",
    "                sum_positive = sum_positive + result[\"scores\"][labels_positive_index]\n",
    "                sum_neutral = sum_neutral + result[\"scores\"][labels_neutral_index]\n",
    "\n",
    "                sentiment_results_list.append(each_score)\n",
    "\n",
    "                # Summarization\n",
    "                if bert_token_estimator.estimate_tokens(chunk)  <= min_token_size:\n",
    "                    corpus_chunks.append(chunk)\n",
    "                else:\n",
    "                    logging.warning(f\"CHUNK, token count: {token_count}\")\n",
    "                    corpus_chunks.append(summarizer_func(chunk))\n",
    "            \n",
    "            # Sentiment\n",
    "            average_negative = sum_negative / i\n",
    "            average_positive = sum_positive / i\n",
    "            average_neutral = sum_neutral / i\n",
    "\n",
    "            score = []\n",
    "            score.append(average_negative)\n",
    "            score.append(average_positive)\n",
    "            score.append(average_neutral)\n",
    "\n",
    "            result_dict = {classifier_labels[i]: score[i] for i in range(len(classifier_labels))}\n",
    "\n",
    "            # builtins required so as not to confuse with pyspark max() function\n",
    "            sentiment_label = builtins.max(result_dict, key=result_dict.get)\n",
    "\n",
    "\n",
    "            # Summarization\n",
    "            s = \" \"\n",
    "            summary_text = s.join(corpus_chunks)\n",
    "        \n",
    "        else:\n",
    "            # Sentiment\n",
    "            result = classifier(r,classifier_labels,multi_label=True)\n",
    "            sentiment_label = sentiment_func(result)\n",
    "\n",
    "            # Summarization\n",
    "            if token_count < min_token_output:\n",
    "                summary_text = r\n",
    "            else:\n",
    "                logging.warning(f\"No chunk, token count: {token_count}\")\n",
    "                summary_text = summarizer_func(r)\n",
    "\n",
    "        bartUpdated_list.append(\"Y\")\n",
    "        sentimentAnalyzed_list.append(\"Y\")\n",
    "        sentiment_list.append(sentiment_label)\n",
    "        bartSummary_list.append(summary_text)\n",
    "        #print(\"bartsummarylist:\", bartSummary_list)\n",
    "        logging.warning(f\"Length of summary list: {len(bartSummary_list)}\")\n",
    "    \n",
    "    bartUpdated_array = np.array([bartUpdated_list])\n",
    "    bartUpdated_concat = np.concatenate(bartUpdated_array)\n",
    "\n",
    "    sentimentAnalyzed_array = np.array([sentimentAnalyzed_list])\n",
    "    sentimentAnalyzed_concat = np.concatenate(sentimentAnalyzed_array)\n",
    "\n",
    "    sentiment_array = np.array([sentiment_list])\n",
    "    sentiment_concat = np.concatenate(sentiment_array)\n",
    "    \n",
    "    bartSummary_array = np.array([bartSummary_list])\n",
    "    bartSummary_concat = np.concatenate(bartSummary_array)\n",
    "\n",
    "    return_df = (\n",
    "        df[[\n",
    "            \"asin\",\"overall\",\"reviewText\",\"reviewTimeTS\",\n",
    "            \"reviewerID\",\"reviewerName\",\"summary\",\"verified\",\n",
    "            \"row_number\",\"asin_key\",\"batch_id\"]]\n",
    "        .assign(bartUpdated=list(bartUpdated_concat))\n",
    "        .assign(sentimentAnalyzed=list(sentimentAnalyzed_concat))\n",
    "        .assign(sentiment=list(sentiment_concat))\n",
    "        .assign(bartSummary=list(bartSummary_concat))\n",
    "    )\n",
    "\n",
    "    count_df_rows = return_df.count()\n",
    "\n",
    "    logging.warning(f\"DF rows: {count_df_rows}\")\n",
    "    \n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e21b07cf-cc41-49d7-9b20-b8f1d1e3dcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_batch(schema, df):\n",
    "\n",
    "    df_summary = ( df\n",
    "      .groupBy(spark_partition_id().alias(\"_pid\"))\n",
    "      .applyInPandas(getPdfLLM,schema)\n",
    "    )\n",
    "\n",
    "    df_summary.write.format(\"delta\").mode(\"append\").save(SINK_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5576137a-fbeb-4113-a597-35219f4783aa",
   "metadata": {},
   "source": [
    "# ForeachBatch definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e2ebce4-7fdb-4aaf-a36d-fe6363914939",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preBatchRecords(microBatchDf, batchSize):\n",
    "    batch_count = math.ceil(microBatchDf.count() / batchSize)\n",
    "    # % sign is modulus -- remainder after division\n",
    "    microBatchDf = microBatchDf.withColumn(\"batch_id\", col(\"row_number\") % batch_count)\n",
    "    microBatchDf = microBatchDf.withColumn(\"bartUpdated\", lit(\"N\"))\n",
    "    microBatchDf = microBatchDf.withColumn(\"sentimentAnalyzed\", lit(\"N\"))\n",
    "    microBatchDf = microBatchDf.withColumn(\"sentiment\", lit(\"\"))\n",
    "    microBatchDf = microBatchDf.withColumn(\"bartSummary\", lit(\"\"))\n",
    "\n",
    "    return microBatchDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b3ccce9-1575-46f0-bf3d-7f1d4319f287",
   "metadata": {},
   "outputs": [],
   "source": [
    "def callUdfBatch(df, batchId):\n",
    "#    path = \"s3a://amzn-customer-reviews-228924278364/sink/test/test-streaming-foreach-pandas/\"\n",
    "    submitted_df = preBatchRecords(df, 5)\n",
    "\n",
    "    ## Temporary to test\n",
    "    #ubmitted_df = submitted_df.limit(10)\n",
    "    #logging.warning(f\"Schema: {submitted_df.printSchema()}\")\n",
    "\n",
    "    ###############\n",
    "\n",
    "    schema = StructType(\n",
    "       [\n",
    "           StructField('asin', StringType(), True),\n",
    "           StructField('overall', DoubleType(), True),\n",
    "           StructField('reviewText', StringType(), True),\n",
    "           StructField('reviewTimeTS', TimestampType(), True),\n",
    "           StructField('reviewerID', StringType(), True),\n",
    "           StructField('reviewerName', StringType(), True),\n",
    "           StructField('summary', StringType(), True),\n",
    "           StructField('verified', BooleanType(), True),\n",
    "           StructField('row_number', IntegerType(), True),\n",
    "           StructField('asin_key', IntegerType(), True),\n",
    "           StructField('batch_id', IntegerType(), True),\n",
    "           StructField(\"bartUpdated\", StringType(), True),\n",
    "           StructField(\"sentimentAnalyzed\", StringType(), True),\n",
    "           StructField(\"sentiment\", StringType(), True), \n",
    "           StructField(\"bartSummary\", StringType(), True)\n",
    "       ]\n",
    "    )\n",
    "\n",
    "    df_total_count = submitted_df.count()\n",
    "    loops = math.ceil(df_total_count / MAX_ROW_SINK)\n",
    "\n",
    "    # Create row_count\n",
    "    submitted_df = submitted_df.withColumn(\"temp_column\", lit(\"A\"))\n",
    "    w = Window().partitionBy(\"temp_column\").orderBy(lit(\"A\"))\n",
    "    submitted_df = submitted_df.withColumn(\"row_id\", row_number().over(w)).drop(\"temp_column\")\n",
    "\n",
    "\n",
    "    logging.warning(f\"Row count of DF total: {df_total_count}\")\n",
    "    logging.warning(f\"Number of loops: {loops}\")\n",
    "\n",
    "    for each_loop in range(loops):\n",
    "        filter_start = 1 + ( MAX_ROW_SINK * each_loop )\n",
    "        \n",
    "        if each_loop == ( loops - 1 ):\n",
    "            filter_end = df_count\n",
    "        else:\n",
    "            filter_end = filter_start + MAX_ROW_SINK\n",
    "\n",
    "        eachloop_df =  submitted_df.filter((col(\"row_id\") >= filter_start) & (col(\"row_id\") <= filter_end))\n",
    "        \n",
    "        logging.warning(f\"Submitted df batch: row start = {filter_start} | row end = {filter_end}\") \n",
    "        llm_batch(schema, eachloop_df)\n",
    "    \n",
    "\n",
    "    #df_summary = ( df\n",
    "    #  .groupBy(spark_partition_id().alias(\"_pid\"))\n",
    "    #  .applyInPandas(getPdfLLM,schema)\n",
    "    #)\n",
    "\n",
    "    #sink_path = \"s3a://amzn-customer-reviews-228924278364/sink/test/test-streaming-bart-summarized/\"\n",
    "    #\n",
    "    ###############\n",
    "\n",
    "#    submitted_df.write.format(\"delta\").mode(\"append\").save(SINK_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ad6280-17d7-47aa-ada7-94c84f449932",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop = 5\n",
    "#for i in range(loop):\n",
    "#    llm_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf05c8ec-f051-49d9-9855-0346c4b854e6",
   "metadata": {},
   "source": [
    "# Reading from Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea8c5ce6-7bd3-4119-a75b-f353d2d495a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting the Kafka options\n",
    "options_read = {\n",
    "    \"kafka.bootstrap.servers\": BOOTSTRAP_SERVERS,\n",
    "    \"subscribe\": TOPIC,\n",
    "    \"startingOffsets\": \"latest\",\n",
    "    \"kafka.security.protocol\": \"SASL_SSL\",\n",
    "    \"kafka.sasl.mechanism\": \"AWS_MSK_IAM\",\n",
    "    \"kafka.sasl.jaas.config\": \"software.amazon.msk.auth.iam.IAMLoginModule required;\",\n",
    "    \"kafka.sasl.client.callback.handler.class\": \"software.amazon.msk.auth.iam.IAMClientCallbackHandler\",\n",
    "    \"maxFilesPerTrigger\": 30\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42b2e56b-667d-4e4b-a053-1567e5ac8b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read from Kafka\n",
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .options(**options_read) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d8e96e-431c-461e-87a5-444b34c526ff",
   "metadata": {},
   "source": [
    "# Writing to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee0ac30c-b5da-4340-8e2e-95df2c215f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting schema of Kafka message topic\n",
    "json_schema = StructType([\n",
    "    StructField('asin', StringType(), True),\n",
    "    StructField('overall', DoubleType(), True),\n",
    "    StructField('reviewText', StringType(), True),\n",
    "    StructField('reviewTimeTS', TimestampType(), True),\n",
    "    StructField('reviewerID', StringType(), True),\n",
    "    StructField('reviewerName', StringType(), True),\n",
    "    StructField('summary', StringType(), True),\n",
    "    StructField('verified', BooleanType(), True),\n",
    "    StructField('row_number', IntegerType(), True),\n",
    "    StructField('asin_key', IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4677b2d-f285-478d-aa9f-8637f4dcae9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/17 17:42:43 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "24/03/17 17:42:44 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "WARNING:root:Row count of DF total: 9761                                        \n",
      "WARNING:root:Number of loops: 326\n",
      "WARNING:root:Submitted df batch: row start = 1 | row end = 31\n",
      "24/03/17 17:43:00 WARN ParquetOutputFormat: Setting parquet.enable.summary-metadata is deprecated, please use parquet.summary.metadata.level\n",
      "2024-03-17 17:43:06.111192: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-17 17:43:06.111243: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-17 17:43:06.112416: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-17 17:43:06.119010: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-17 17:43:06.926183: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "WARNING:root:Length of summary list: 1\n",
      "WARNING:root:Length of summary list: 2\n",
      "WARNING:root:Length of summary list: 3\n",
      "WARNING:root:Length of summary list: 4\n",
      "WARNING:root:Length of summary list: 5\n",
      "WARNING:root:Length of summary list: 6\n",
      "WARNING:root:Length of summary list: 7\n",
      "WARNING:root:Length of summary list: 8\n",
      "WARNING:root:Length of summary list: 9\n",
      "WARNING:root:Length of summary list: 10\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "WARNING:root:Length of summary list: 11\n",
      "WARNING:root:Length of summary list: 12\n",
      "WARNING:root:Length of summary list: 13\n",
      "WARNING:root:Length of summary list: 14\n",
      "WARNING:root:Length of summary list: 15\n",
      "WARNING:root:Length of summary list: 16\n",
      "WARNING:root:Length of summary list: 17\n",
      "WARNING:root:Length of summary list: 18\n",
      "WARNING:root:No chunk, token count: 239\n",
      "WARNING:root:Length of summary list: 19\n",
      "WARNING:root:Length of summary list: 20\n",
      "WARNING:root:Length of summary list: 21\n",
      "WARNING:root:Length of summary list: 22\n",
      "WARNING:root:Length of summary list: 23\n",
      "WARNING:root:No chunk, token count: 148\n",
      "WARNING:root:Length of summary list: 24\n",
      "WARNING:root:Length of summary list: 25\n",
      "WARNING:root:Length of summary list: 26\n",
      "WARNING:root:Length of summary list: 27\n",
      "WARNING:root:Length of summary list: 28\n",
      "WARNING:root:Length of summary list: 29\n",
      "WARNING:root:Length of summary list: 30\n",
      "WARNING:root:Length of summary list: 31\n",
      "WARNING:root:DF rows: asin                 31\n",
      "overall              31\n",
      "reviewText           31\n",
      "reviewTimeTS         31\n",
      "reviewerID           31\n",
      "reviewerName         31\n",
      "summary              31\n",
      "verified             31\n",
      "row_number           31\n",
      "asin_key             26\n",
      "batch_id             31\n",
      "bartUpdated          31\n",
      "sentimentAnalyzed    31\n",
      "sentiment            31\n",
      "bartSummary          31\n",
      "dtype: int64\n",
      "24/03/17 17:43:17 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "WARNING:root:Submitted df batch: row start = 31 | row end = 61                  \n",
      "24/03/17 17:43:28 WARN ParquetOutputFormat: Setting parquet.enable.summary-metadata is deprecated, please use parquet.summary.metadata.level\n",
      "WARNING:root:Length of summary list: 1=========================>(199 + 1) / 200]\n",
      "WARNING:root:Length of summary list: 2\n",
      "WARNING:root:No chunk, token count: 361\n",
      "WARNING:root:Length of summary list: 3\n",
      "WARNING:root:Length of summary list: 4\n",
      "WARNING:root:Length of summary list: 5\n",
      "WARNING:root:Length of summary list: 6\n",
      "WARNING:root:Length of summary list: 7\n",
      "WARNING:root:No chunk, token count: 273\n",
      "WARNING:root:Length of summary list: 8\n",
      "WARNING:root:Length of summary list: 9\n",
      "WARNING:root:Length of summary list: 10\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "WARNING:root:Length of summary list: 11\n",
      "WARNING:root:Length of summary list: 12\n",
      "WARNING:root:Length of summary list: 13\n",
      "WARNING:root:Length of summary list: 14\n",
      "WARNING:root:Length of summary list: 15\n",
      "WARNING:root:Length of summary list: 16\n",
      "WARNING:root:Length of summary list: 17\n",
      "WARNING:root:Length of summary list: 18\n",
      "WARNING:root:Length of summary list: 19\n",
      "WARNING:root:Length of summary list: 20\n",
      "WARNING:root:Length of summary list: 21\n",
      "WARNING:root:Length of summary list: 22\n",
      "WARNING:root:Length of summary list: 23\n",
      "WARNING:root:Length of summary list: 24\n",
      "WARNING:root:Length of summary list: 25\n",
      "WARNING:root:Length of summary list: 26\n",
      "WARNING:root:Length of summary list: 27\n",
      "WARNING:root:No chunk, token count: 208\n",
      "WARNING:root:Length of summary list: 28\n",
      "WARNING:root:No chunk, token count: 133\n",
      "WARNING:root:Length of summary list: 29\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (659 > 512). Running this sequence through the model will result in indexing errors\n",
      "WARNING:root:No chunk, token count: 659\n",
      "WARNING:root:Length of summary list: 30\n",
      "WARNING:root:Length of summary list: 31\n",
      "WARNING:root:DF rows: asin                 31\n",
      "overall              31\n",
      "reviewText           31\n",
      "reviewTimeTS         31\n",
      "reviewerID           31\n",
      "reviewerName         31\n",
      "summary              31\n",
      "verified             31\n",
      "row_number           31\n",
      "asin_key             25\n",
      "batch_id             31\n",
      "bartUpdated          31\n",
      "sentimentAnalyzed    31\n",
      "sentiment            31\n",
      "bartSummary          31\n",
      "dtype: int64\n",
      "WARNING:root:Submitted df batch: row start = 61 | row end = 91                  \n",
      "24/03/17 17:43:47 WARN ParquetOutputFormat: Setting parquet.enable.summary-metadata is deprecated, please use parquet.summary.metadata.level\n",
      "WARNING:root:Length of summary list: 1=========================>(199 + 1) / 200]\n",
      "WARNING:root:Length of summary list: 2\n",
      "WARNING:root:Length of summary list: 3\n",
      "WARNING:root:Length of summary list: 4\n",
      "WARNING:root:No chunk, token count: 266\n",
      "WARNING:root:Length of summary list: 5\n",
      "WARNING:root:Length of summary list: 6\n",
      "WARNING:root:Length of summary list: 7\n",
      "WARNING:root:Length of summary list: 8\n",
      "WARNING:root:Length of summary list: 9\n",
      "WARNING:root:Length of summary list: 10\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "WARNING:root:Length of summary list: 11\n",
      "WARNING:root:Length of summary list: 12\n",
      "WARNING:root:Length of summary list: 13\n",
      "WARNING:root:Length of summary list: 14\n",
      "WARNING:root:Length of summary list: 15\n",
      "WARNING:root:Length of summary list: 16\n",
      "WARNING:root:Length of summary list: 17\n",
      "WARNING:root:Length of summary list: 18\n",
      "WARNING:root:Length of summary list: 19\n",
      "WARNING:root:Length of summary list: 20\n",
      "WARNING:root:Length of summary list: 21\n",
      "WARNING:root:Length of summary list: 22\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (544 > 512). Running this sequence through the model will result in indexing errors\n",
      "WARNING:root:No chunk, token count: 544\n",
      "WARNING:root:Length of summary list: 23\n",
      "WARNING:root:Length of summary list: 24\n",
      "WARNING:root:Length of summary list: 25\n",
      "WARNING:root:No chunk, token count: 148\n",
      "WARNING:root:Length of summary list: 26\n",
      "WARNING:root:No chunk, token count: 256\n",
      "WARNING:root:Length of summary list: 27\n",
      "WARNING:root:Length of summary list: 28\n",
      "WARNING:root:No chunk, token count: 136\n",
      "WARNING:root:Length of summary list: 29\n",
      "WARNING:root:Length of summary list: 30\n",
      "WARNING:root:Length of summary list: 31\n",
      "WARNING:root:DF rows: asin                 31\n",
      "overall              31\n",
      "reviewText           31\n",
      "reviewTimeTS         31\n",
      "reviewerID           31\n",
      "reviewerName         31\n",
      "summary              31\n",
      "verified             31\n",
      "row_number           31\n",
      "asin_key             21\n",
      "batch_id             31\n",
      "bartUpdated          31\n",
      "sentimentAnalyzed    31\n",
      "sentiment            31\n",
      "bartSummary          31\n",
      "dtype: int64\n",
      "WARNING:root:Submitted df batch: row start = 91 | row end = 121                 \n",
      "24/03/17 17:44:06 WARN ParquetOutputFormat: Setting parquet.enable.summary-metadata is deprecated, please use parquet.summary.metadata.level\n",
      "WARNING:root:Submitted df batch: row start = 121 | row end = 151\n",
      "WARNING:root:Length of summary list: 1=========================>(199 + 1) / 200]\n",
      "WARNING:root:Length of summary list: 2\n",
      "WARNING:root:Length of summary list: 3\n",
      "WARNING:root:Length of summary list: 4\n",
      "WARNING:root:Length of summary list: 5\n",
      "WARNING:root:No chunk, token count: 350\n",
      "WARNING:root:Length of summary list: 6\n",
      "WARNING:root:Length of summary list: 7\n",
      "WARNING:root:Length of summary list: 8\n",
      "WARNING:root:No chunk, token count: 137\n",
      "WARNING:root:Length of summary list: 9\n",
      "WARNING:root:Length of summary list: 10\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "WARNING:root:Length of summary list: 11\n",
      "WARNING:root:Length of summary list: 12\n",
      "WARNING:root:Length of summary list: 13\n",
      "WARNING:root:Length of summary list: 14\n",
      "WARNING:root:Length of summary list: 15\n",
      "WARNING:root:Length of summary list: 16\n",
      "WARNING:root:Length of summary list: 17\n",
      "WARNING:root:Length of summary list: 18\n",
      "WARNING:root:Length of summary list: 19\n",
      "WARNING:root:No chunk, token count: 491\n",
      "WARNING:root:Length of summary list: 20\n",
      "WARNING:root:Length of summary list: 21\n",
      "WARNING:root:Length of summary list: 22\n",
      "WARNING:root:No chunk, token count: 300\n",
      "WARNING:root:Length of summary list: 23\n",
      "WARNING:root:Length of summary list: 24\n",
      "WARNING:root:No chunk, token count: 152\n",
      "WARNING:root:Length of summary list: 25\n",
      "WARNING:root:Length of summary list: 26\n",
      "WARNING:root:Length of summary list: 27\n",
      "WARNING:root:Length of summary list: 28\n",
      "WARNING:root:Length of summary list: 29\n",
      "WARNING:root:Length of summary list: 30\n",
      "WARNING:root:No chunk, token count: 140\n",
      "WARNING:root:Length of summary list: 31\n",
      "WARNING:root:DF rows: asin                 31\n",
      "overall              31\n",
      "reviewText           31\n",
      "reviewTimeTS         31\n",
      "reviewerID           31\n",
      "reviewerName         31\n",
      "summary              31\n",
      "verified             31\n",
      "row_number           31\n",
      "asin_key             24\n",
      "batch_id             31\n",
      "bartUpdated          31\n",
      "sentimentAnalyzed    31\n",
      "sentiment            31\n",
      "bartSummary          31\n",
      "dtype: int64\n",
      "24/03/17 17:44:26 WARN ParquetOutputFormat: Setting parquet.enable.summary-metadata is deprecated, please use parquet.summary.metadata.level\n",
      "WARNING:root:No chunk, token count: 140========================>(199 + 1) / 200]\n",
      "WARNING:root:Length of summary list: 1\n",
      "WARNING:root:No chunk, token count: 389\n",
      "WARNING:root:Length of summary list: 2\n",
      "WARNING:root:Length of summary list: 3\n",
      "WARNING:root:Length of summary list: 4\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (808 > 512). Running this sequence through the model will result in indexing errors\n",
      "WARNING:root:No chunk, token count: 808\n",
      "WARNING:root:Length of summary list: 5\n",
      "WARNING:root:Length of summary list: 6\n",
      "WARNING:root:Length of summary list: 7\n",
      "WARNING:root:Length of summary list: 8\n",
      "WARNING:root:Length of summary list: 9\n",
      "WARNING:root:Length of summary list: 10\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "WARNING:root:Length of summary list: 11\n",
      "WARNING:root:No chunk, token count: 195\n",
      "WARNING:root:Length of summary list: 12\n",
      "WARNING:root:Length of summary list: 13\n",
      "WARNING:root:Length of summary list: 14\n",
      "WARNING:root:No chunk, token count: 203\n",
      "WARNING:root:Length of summary list: 15\n",
      "WARNING:root:Length of summary list: 16\n",
      "WARNING:root:Length of summary list: 17\n",
      "WARNING:root:Length of summary list: 18\n",
      "WARNING:root:Length of summary list: 19\n",
      "WARNING:root:Length of summary list: 20\n",
      "WARNING:root:Length of summary list: 21\n",
      "WARNING:root:No chunk, token count: 638\n",
      "WARNING:root:Length of summary list: 22\n",
      "WARNING:root:No chunk, token count: 164\n",
      "WARNING:root:Length of summary list: 23\n",
      "WARNING:root:No chunk, token count: 419\n",
      "WARNING:root:Length of summary list: 24\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1212 > 512). Running this sequence through the model will result in indexing errors\n",
      "WARNING:root:CHUNK, token count: 1212\n",
      "WARNING:root:CHUNK, token count: 1212\n",
      "WARNING:root:CHUNK, token count: 1212\n",
      "WARNING:root:Length of summary list: 25\n",
      "WARNING:root:Length of summary list: 26\n",
      "WARNING:root:Length of summary list: 27\n",
      "WARNING:root:No chunk, token count: 589\n",
      "WARNING:root:Length of summary list: 28\n",
      "WARNING:root:Length of summary list: 29\n",
      "WARNING:root:Length of summary list: 30\n",
      "WARNING:root:Length of summary list: 31\n",
      "WARNING:root:DF rows: asin                 31\n",
      "overall              31\n",
      "reviewText           31\n",
      "reviewTimeTS         31\n",
      "reviewerID           31\n",
      "reviewerName         31\n",
      "summary              31\n",
      "verified             31\n",
      "row_number           31\n",
      "asin_key             29\n",
      "batch_id             31\n",
      "bartUpdated          31\n",
      "sentimentAnalyzed    31\n",
      "sentiment            31\n",
      "bartSummary          31\n",
      "dtype: int64\n",
      "WARNING:root:Submitted df batch: row start = 151 | row end = 181                \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/broadcast.py\", line 183, in dump\n",
      "    pickle.dump(value, f, pickle_protocol)\n",
      "  File \"/usr/lib/python3.10/tempfile.py\", line 622, in func_wrapper\n",
      "    return func(*args, **kwargs)\n",
      "OSError: [Errno 28] No space left on device\n",
      "ERROR:py4j.clientserver:There was an exception while executing the Python Proxy on the Python Side.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/broadcast.py\", line 183, in dump\n",
      "    pickle.dump(value, f, pickle_protocol)\n",
      "  File \"/usr/lib/python3.10/tempfile.py\", line 622, in func_wrapper\n",
      "    return func(*args, **kwargs)\n",
      "OSError: [Errno 28] No space left on device\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 617, in _call_proxy\n",
      "    return_value = getattr(self.pool[obj_id], method)(*params)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/sql/utils.py\", line 120, in call\n",
      "    raise e\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/sql/utils.py\", line 117, in call\n",
      "    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)\n",
      "  File \"/tmp/ipykernel_169653/4163429986.py\", line 54, in callUdfBatch\n",
      "    llm_batch(schema, eachloop_df)\n",
      "  File \"/tmp/ipykernel_169653/897389054.py\", line 5, in llm_batch\n",
      "    .applyInPandas(getPdfLLM,schema)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/sql/pandas/group_ops.py\", line 230, in applyInPandas\n",
      "    udf_column = udf(*[df[col] for col in df.columns])\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/sql/udf.py\", line 425, in wrapper\n",
      "    return self(*args)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/sql/udf.py\", line 402, in __call__\n",
      "    judf = self._judf\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/sql/udf.py\", line 322, in _judf\n",
      "    self._judf_placeholder = self._create_judf(self.func)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/sql/udf.py\", line 331, in _create_judf\n",
      "    wrapped_func = _wrap_function(sc, func, self.returnType)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/sql/udf.py\", line 60, in _wrap_function\n",
      "    pickled_command, broadcast_vars, env, includes = _prepare_for_python_RDD(sc, command)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/rdd.py\", line 5255, in _prepare_for_python_RDD\n",
      "    broadcast = sc.broadcast(pickled_command)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/context.py\", line 1765, in broadcast\n",
      "    return Broadcast(self, value, self._pickled_broadcast_vars)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/broadcast.py\", line 135, in __init__\n",
      "    self.dump(value, broadcast_out)  # type: ignore[arg-type]\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/broadcast.py\", line 189, in dump\n",
      "    raise pickle.PicklingError(msg)\n",
      "_pickle.PicklingError: Could not serialize broadcast: OSError: [Errno 28] No space left on device\n",
      "24/03/17 17:44:52 ERROR MicroBatchExecution: Query [id = e938420b-8b04-4515-b3fa-3e68ee1a7dd0, runId = 3c37b7df-e0b4-4e85-81ea-86ba722ea8b1] terminated with error\n",
      "py4j.Py4JException: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/broadcast.py\", line 183, in dump\n",
      "    pickle.dump(value, f, pickle_protocol)\n",
      "  File \"/usr/lib/python3.10/tempfile.py\", line 622, in func_wrapper\n",
      "    return func(*args, **kwargs)\n",
      "OSError: [Errno 28] No space left on device\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 617, in _call_proxy\n",
      "    return_value = getattr(self.pool[obj_id], method)(*params)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/sql/utils.py\", line 120, in call\n",
      "    raise e\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/sql/utils.py\", line 117, in call\n",
      "    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)\n",
      "  File \"/tmp/ipykernel_169653/4163429986.py\", line 54, in callUdfBatch\n",
      "    llm_batch(schema, eachloop_df)\n",
      "  File \"/tmp/ipykernel_169653/897389054.py\", line 5, in llm_batch\n",
      "    .applyInPandas(getPdfLLM,schema)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/sql/pandas/group_ops.py\", line 230, in applyInPandas\n",
      "    udf_column = udf(*[df[col] for col in df.columns])\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/sql/udf.py\", line 425, in wrapper\n",
      "    return self(*args)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/sql/udf.py\", line 402, in __call__\n",
      "    judf = self._judf\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/sql/udf.py\", line 322, in _judf\n",
      "    self._judf_placeholder = self._create_judf(self.func)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/sql/udf.py\", line 331, in _create_judf\n",
      "    wrapped_func = _wrap_function(sc, func, self.returnType)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/sql/udf.py\", line 60, in _wrap_function\n",
      "    pickled_command, broadcast_vars, env, includes = _prepare_for_python_RDD(sc, command)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/rdd.py\", line 5255, in _prepare_for_python_RDD\n",
      "    broadcast = sc.broadcast(pickled_command)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/context.py\", line 1765, in broadcast\n",
      "    return Broadcast(self, value, self._pickled_broadcast_vars)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/broadcast.py\", line 135, in __init__\n",
      "    self.dump(value, broadcast_out)  # type: ignore[arg-type]\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/broadcast.py\", line 189, in dump\n",
      "    raise pickle.PicklingError(msg)\n",
      "_pickle.PicklingError: Could not serialize broadcast: OSError: [Errno 28] No space left on device\n",
      "\n",
      "\tat py4j.Protocol.getReturnValue(Protocol.java:476)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:108)\n",
      "\tat jdk.proxy3/jdk.proxy3.$Proxy30.call(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.stre"
     ]
    },
    {
     "ename": "StreamingQueryException",
     "evalue": "[STREAM_FAILED] Query [id = e938420b-8b04-4515-b3fa-3e68ee1a7dd0, runId = 3c37b7df-e0b4-4e85-81ea-86ba722ea8b1] terminated with exception: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/broadcast.py\", line 183, in dump\n    pickle.dump(value, f, pickle_protocol)\n  File \"/usr/lib/python3.10/tempfile.py\", line 622, in func_wrapper\n    return func(*args, **kwargs)\nOSError: [Errno 28] No space left on device\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 617, in _call_proxy\n    return_value = getattr(self.pool[obj_id], method)(*params)\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/sql/utils.py\", line 120, in call\n    raise e\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/sql/utils.py\", line 117, in call\n    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)\n  File \"/tmp/ipykernel_169653/4163429986.py\", line 54, in callUdfBatch\n    llm_batch(schema, eachloop_df)\n  File \"/tmp/ipykernel_169653/897389054.py\", line 5, in llm_batch\n    .applyInPandas(getPdfLLM,schema)\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/sql/pandas/group_ops.py\", line 230, in applyInPandas\n    udf_column = udf(*[df[col] for col in df.columns])\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/sql/udf.py\", line 425, in wrapper\n    return self(*args)\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/sql/udf.py\", line 402, in __call__\n    judf = self._judf\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/sql/udf.py\", line 322, in _judf\n    self._judf_placeholder = self._create_judf(self.func)\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/sql/udf.py\", line 331, in _create_judf\n    wrapped_func = _wrap_function(sc, func, self.returnType)\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/sql/udf.py\", line 60, in _wrap_function\n    pickled_command, broadcast_vars, env, includes = _prepare_for_python_RDD(sc, command)\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/rdd.py\", line 5255, in _prepare_for_python_RDD\n    broadcast = sc.broadcast(pickled_command)\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/context.py\", line 1765, in broadcast\n    return Broadcast(self, value, self._pickled_broadcast_vars)\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/broadcast.py\", line 135, in __init__\n    self.dump(value, broadcast_out)  # type: ignore[arg-type]\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/broadcast.py\", line 189, in dump\n    raise pickle.PicklingError(msg)\n_pickle.PicklingError: Could not serialize broadcast: OSError: [Errno 28] No space left on device\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStreamingQueryException\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 13\u001b[0m\n\u001b[1;32m      1\u001b[0m streamHandle \u001b[38;5;241m=\u001b[39m (df\n\u001b[1;32m      2\u001b[0m                 \u001b[38;5;241m.\u001b[39mselectExpr(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCAST(key as STRING)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCAST(value as STRING)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m                 \u001b[38;5;241m.\u001b[39mselect(from_json(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m,json_schema)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata.*\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m                 \u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m      9\u001b[0m                )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#                \u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[43mstreamHandle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/streaming/query.py:221\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mStreamingQueryException\u001b[0m: [STREAM_FAILED] Query [id = e938420b-8b04-4515-b3fa-3e68ee1a7dd0, runId = 3c37b7df-e0b4-4e85-81ea-86ba722ea8b1] terminated with exception: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/broadcast.py\", line 183, in dump\n    pickle.dump(value, f, pickle_protocol)\n  File \"/usr/lib/python3.10/tempfile.py\", line 622, in func_wrapper\n    return func(*args, **kwargs)\nOSError: [Errno 28] No space left on device\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 617, in _call_proxy\n    return_value = getattr(self.pool[obj_id], method)(*params)\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/sql/utils.py\", line 120, in call\n    raise e\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/sql/utils.py\", line 117, in call\n    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)\n  File \"/tmp/ipykernel_169653/4163429986.py\", line 54, in callUdfBatch\n    llm_batch(schema, eachloop_df)\n  File \"/tmp/ipykernel_169653/897389054.py\", line 5, in llm_batch\n    .applyInPandas(getPdfLLM,schema)\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/sql/pandas/group_ops.py\", line 230, in applyInPandas\n    udf_column = udf(*[df[col] for col in df.columns])\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/sql/udf.py\", line 425, in wrapper\n    return self(*args)\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/sql/udf.py\", line 402, in __call__\n    judf = self._judf\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/sql/udf.py\", line 322, in _judf\n    self._judf_placeholder = self._create_judf(self.func)\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/sql/udf.py\", line 331, in _create_judf\n    wrapped_func = _wrap_function(sc, func, self.returnType)\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/sql/udf.py\", line 60, in _wrap_function\n    pickled_command, broadcast_vars, env, includes = _prepare_for_python_RDD(sc, command)\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/rdd.py\", line 5255, in _prepare_for_python_RDD\n    broadcast = sc.broadcast(pickled_command)\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/context.py\", line 1765, in broadcast\n    return Broadcast(self, value, self._pickled_broadcast_vars)\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/broadcast.py\", line 135, in __init__\n    self.dump(value, broadcast_out)  # type: ignore[arg-type]\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/broadcast.py\", line 189, in dump\n    raise pickle.PicklingError(msg)\n_pickle.PicklingError: Could not serialize broadcast: OSError: [Errno 28] No space left on device\n"
     ]
    }
   ],
   "source": [
    "streamHandle = (df\n",
    "                .selectExpr(\"CAST(key as STRING)\",\"CAST(value as STRING)\")\n",
    "                .select(from_json(\"value\",json_schema).alias(\"data\")).select(\"data.*\")\n",
    "                .writeStream\n",
    "                .foreachBatch(callUdfBatch)\n",
    "                .option(\"checkpointLocation\", CHECKPOINT_PATH)\n",
    "                .trigger(processingTime='10 seconds')\n",
    "                .start()\n",
    "               )\n",
    "\n",
    "#                \n",
    "\n",
    "streamHandle.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ff3dac-4c86-4984-84b3-aefbbbfddebb",
   "metadata": {},
   "source": [
    "# View results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6087174-ee60-4a8c-9dfe-806f4bb47ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check = spark.read.format(\"delta\").load(SINK_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08eca0ea-8d9f-452a-ae5f-45aa8641f074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- asin: string (nullable = true)\n",
      " |-- overall: double (nullable = true)\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- reviewTimeTS: timestamp (nullable = true)\n",
      " |-- reviewerID: string (nullable = true)\n",
      " |-- reviewerName: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- verified: boolean (nullable = true)\n",
      " |-- row_number: integer (nullable = true)\n",
      " |-- asin_key: integer (nullable = true)\n",
      " |-- batch_id: integer (nullable = true)\n",
      " |-- bartUpdated: string (nullable = true)\n",
      " |-- sentimentAnalyzed: string (nullable = true)\n",
      " |-- sentiment: string (nullable = true)\n",
      " |-- bartSummary: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_check.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835bfc67-5c0f-4ddd-a92f-acce4c95def9",
   "metadata": {},
   "source": [
    "# Count difference between summarized review and actual review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cfaa50cf-486b-4524-9d59-7e0d7a0b20f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary = ( df_check.select(\"asin_key\",length(df_check.reviewText).alias(\"lengthText\"),\n",
    "                           length(df_check.bartSummary).alias(\"lengthBartSummary\"),\"bartSummary\",\n",
    "                ( length(df_check.reviewText) - length(df_check.bartSummary) ).alias(\"lengthDiff\"))\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4a67b73-8356-4bc8-a263-47cf6c241379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----------------+--------------------+----------+\n",
      "| asin_key|lengthText|lengthBartSummary|         bartSummary|lengthDiff|\n",
      "+---------+----------+-----------------+--------------------+----------+\n",
      "| 62697439|      6005|              818|A liberal Democra...|      5187|\n",
      "| 62655760|      3454|              386|Cassandra Holwell...|      3068|\n",
      "| 61173754|      3110|              352|Let Us Compare My...|      2758|\n",
      "| 62663674|      2832|              299|True Stories from...|      2533|\n",
      "|     NULL|      2620|              165|White Hot is book...|      2455|\n",
      "|125643202|      2710|              360|Charity Church ha...|      2350|\n",
      "| 62448110|      2172|              221|I almost stopped ...|      1951|\n",
      "|125384976|      1947|              314|Lenora Bell uses ...|      1633|\n",
      "| 62427059|      1860|              290|Swanson uses four...|      1570|\n",
      "|  7548672|      1650|              195|The prose was won...|      1455|\n",
      "|     NULL|      1427|              201|Although I'm much...|      1226|\n",
      "|     NULL|      1354|              220|\"Consider Emmanue...|      1134|\n",
      "|     NULL|      1278|              223|The War of 1812 i...|      1055|\n",
      "|124676112|      1204|              217|A gripping page t...|       987|\n",
      "|  8131996|      1092|              294|I honestly don't ...|       798|\n",
      "|  8228337|       966|              192|First off, this b...|       774|\n",
      "| 61050369|       902|              191|The first major L...|       711|\n",
      "| 62676083|       891|              218|The name comes fr...|       673|\n",
      "|188035254|       863|              267|This book is drea...|       596|\n",
      "| 62674552|       736|              193| Moonlight Sins w...|       543|\n",
      "+---------+----------+-----------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_summary.sort(df_summary.lengthDiff.desc()).show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1e6d88-63d4-4c20-8871-2807efcedd52",
   "metadata": {},
   "source": [
    "# View review text and summary in full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c78edc3-9308-4010-a756-1a83c08b4153",
   "metadata": {},
   "outputs": [],
   "source": [
    "asin_key_view = 62697439"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee7b4b35-fc9c-40e9-9bcc-55598161383f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_view_summary = (\n",
    "    df_check.filter(f\"asin_key=={asin_key_view}\").distinct()\n",
    "       .select(\"asin_key\",\"reviewText\",\"bartSummary\")\n",
    "       .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc1a86c-5299-4f3c-963c-e03a845a4086",
   "metadata": {},
   "source": [
    "# Original text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8fc36179-9bf3-40ed-8559-6b82f71948d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This book by a liberal Democrat attempts to diagnose the partys failure in the 2016 election and tentatively proposes a remedy. His diagnosis is presented in terms of two dispensations that he finds in American politics since the New Deal.  The word refers to what my dictionary calls a divinely ordained order prevailing at a particular period in history. (The author does not discuss the question of whether the orders he describes were divinely ordained.) To me, his diagnosis seems accurate. But at the end, when he proposes a remedy, he is not convincing. He and I have different political orientations. His book gave me an excellent understanding of the problems liberals face although it is not what he evidently intended.\n",
      "\n",
      "The first or Roosevelt dispensation began with the New Deal and ended in 1980 with the defeat of Carter, who is described as disjunctive, marking the end of the period. Then began the Reagan dispensation which is now coming to an end (the author hopes) with Trump. The crucial element in each of these eras is a conjunction of major events with attitudes in the electorate. For the Roosevelt period, the events were the Wall Street crash and the coming of fascism; the primary attitudes were fear of financial ruin and war. The consequence was a desire for social solidarity in the face of danger. Over the years, there was a continual feeling of unified social progress as racial integration and civil rights. (To this I would add the emergence of the US as the world power. But the components of a dispensation can be elusive.)\n",
      "\n",
      "The Roosevelt dispensation came to an end in the late 70s largely due to the energy crisis, Jimmy Carters own cramped personality with its emphasis on conservation of energy (keep that thermostat down), and the general economic stagnation caused by high oil prices. To these I would add the Iranian revolution and the hostage crisis. In contrast to Carter, Reagan provided a common vision of what America was and could become, so that the party ceased to be a coalition and became an ideologically unified and electorally potent force that thought and acted like a fine-tuned machine. The Reagan dispensation was greatly strengthened as the result of careful steps conservatives took in preparing for their new age by creating think tanks outside the university. There were academics like Robert Nozick who in his Anarchy, State, and Utopia, presented the doctrine of old-fashioned American individualism in a form that did not demand great sacrifices. As Grover Norquist put it, it wasn't necessary to have a President auditioning for fearless leader when conservatives knew in what direction to go. Thus conservatives showed that it was possible for intellectuals to plan the elements of a successful dispensation, even if a sufficiently resourceful or fortunate president could run against the current of his time and still succeed, as the examples of Eisenhower, Clinton, and Obama show. (The author has very little to say about these exceptional cases.)\n",
      "\n",
      "During these years, liberals were losing common sense. American cultural changes made the universities into the locus of liberal politics, and the word identity acquired a near-mystical potency, which the author summarizes amusingly: ...  [The]  retreating New Left turned the university into a pseudo-political theater for the staging of operas and melodramas. The author, a Professor of Humanities at Columbia, discusses the excesses of identity politics at length. It was difficult to create a mass movement out of all the identities of blacks, whites, women, men, gays and so on. Even more important, in the world of real-life politics, liberals ended up losing.\n",
      "\n",
      "It stands to reason that if liberals want their point of view to dominate, they should make some effort to demonstrate it and prove it. But the author takes for granted that his usual liberal point of view is right. It is unconscionable that black motorists and pedestrians have been regularly singled out by police officers who then handled them violently, with impunity in some places. An example like that of Michael Brown proves nothing except a lack of facts and that the police can be expected to be biased in their own favor.  He doesnt even mention the bogus sexual assault charges on college campuses, causing students to be expelled from college --  the result of regulations promulgated by Federal officials. Apparently he wants officials like police or accused students to be biased against themselves. Not surprisingly, he admires conservatives for their political cleverness, while not acknowledging that liberals do not have facts on their side.\n",
      "\n",
      "In the last chapter, the writer looks for a solution to the current social fragmentation, trying to find a basis for consensus.  He declares that a we must be formed out of the competing identities. The author thinks a we is to be found in the concept of citizenship. All citizens have basic rights and obligations.  Few can disagree here. But who is a citizen?  He refuses to answer:  ... any mention of the term citizen leads people to think of the hypocritical and racist demagoguery that passes for our debate on immigration and refugees today. I will not be discussing such matters here .... If discussion is hypocritical and racist, his position is pretty clear: open borders, with the US a geographical and legal entity but nothing more. The voters disagree with him.\n",
      "\n",
      "Once we have defined \"citizens\", what are their rights and obligations? He continues In the twentieth century, the question [of citizenship] centered on what was materially necessary to enjoy the benefits of democratic citizenship equally, which provided a way of making the case for the modern welfare state. (p. 122) This seems to mean that everyone is assumed to be a citizen and therefore entitlements are deserved by right -- a circular argument. Here too I think a clear majority of voters will disagree. Liberals have a long way to go and do not know how wrong they are.\n"
     ]
    }
   ],
   "source": [
    "print(df_view_summary[0][\"reviewText\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cda398c-8b7b-47f1-a00b-f9b31fc0c4a0",
   "metadata": {},
   "source": [
    "# View summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2f861b7d-79a6-4767-b735-62f36ba516f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A liberal Democrat attempts to diagnose the partys failure in the 2016 election and tentatively proposes a remedy. His diagnosis is presented in terms of two dispensations that he finds in American politics since the New Deal. The crucial element in each of these eras is a conjunction of major events with attitudes. The author, a Professor of Humanities at Columbia, discusses the excesses of identity politics at length. He admires conservatives for their political cleverness, while not acknowledging that liberals do not have facts on their side. In the last chapter, the writer looks for a solution to the current social fragmentation, trying to find a basis for consensus. He declares that a we must be formed out of the competing identities. The author thinks a we is to be found in the concept of citizenship.\n"
     ]
    }
   ],
   "source": [
    "print(df_view_summary[0][\"bartSummary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe31627-0fc3-4ddf-864e-5e3486f91002",
   "metadata": {},
   "source": [
    "# View Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d23a135b-968f-4d12-9aac-c551088e056b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+--------------------+---------+\n",
      "|asin_key|overall|          reviewtext|sentiment|\n",
      "+--------+-------+--------------------+---------+\n",
      "|60262141|    5.0|Our granddaughter...| positive|\n",
      "|    NULL|    5.0|I loved this book...| positive|\n",
      "| 7548672|    4.0|Overall, I found ...| negative|\n",
      "|60526157|    1.0|If you enjoyed wh...| negative|\n",
      "|60554738|    5.0|A great book abou...| positive|\n",
      "|    NULL|    5.0|Last read the boo...| positive|\n",
      "|60595183|    5.0|Good purchase gre...| positive|\n",
      "| 8131996|    5.0|I honestly don't ...| positive|\n",
      "|    NULL|    5.0|         Great Book!| positive|\n",
      "|60746394|    5.0|the film did not ...| negative|\n",
      "+--------+-------+--------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sentiment = df_check.select(\"asin_key\",\"overall\",\"reviewtext\",\"sentiment\")\n",
    "df_sentiment.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "24e3fe78-4ac6-4598-8425-b76e7fea2365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|sentiment|count|\n",
      "+---------+-----+\n",
      "| positive|  119|\n",
      "| negative|   36|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sentiment.groupBy(\"sentiment\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9bc3bda4-6aaf-47be-b2ca-cc347cda06b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-----+\n",
      "|overall|sentiment|count|\n",
      "+-------+---------+-----+\n",
      "|    5.0| positive|   92|\n",
      "|    5.0| negative|    6|\n",
      "|    4.0| positive|   23|\n",
      "|    4.0| negative|   10|\n",
      "|    3.0| positive|    4|\n",
      "|    3.0| negative|    8|\n",
      "|    2.0| negative|    5|\n",
      "|    1.0| negative|    7|\n",
      "+-------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sentiment.groupBy(\"overall\",\"sentiment\").count().orderBy(desc(\"overall\"),desc(\"sentiment\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86679f7c-98b9-402b-97d1-817a38908a94",
   "metadata": {},
   "source": [
    "# Looking at a positive sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "648af317-ae51-49a3-99ff-de5dcebe4639",
   "metadata": {},
   "outputs": [],
   "source": [
    "asin_key_check = 60262141\n",
    "df_sentiment_check = df_sentiment.select(\"reviewText\",\"overall\",\"sentiment\") \\\n",
    "   .filter(f\"asin_key == {asin_key_check}\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "95062cfc-e375-4180-8e3f-d2c4498fcb3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer indicated score: 5.0 | Sentiment: positive\n",
      "======================\n",
      "Our granddaughter, Sweet Emma, lives in Barcelona & is being raised bilingual.\n",
      "Perfect!\n"
     ]
    }
   ],
   "source": [
    "checktext = df_sentiment_check[0][\"reviewText\"]\n",
    "checkoverall = df_sentiment_check[0][\"overall\"]\n",
    "checksentiment = df_sentiment_check[0][\"sentiment\"]\n",
    "print(f\"Customer indicated score: {checkoverall} | Sentiment: {checksentiment}\")\n",
    "print(\"======================\")\n",
    "print(checktext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e88973f-b068-4847-8339-e763d94ff59b",
   "metadata": {},
   "source": [
    "# Looking at a negative sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "961c9d87-5d2f-434c-8682-a12ecaf2767c",
   "metadata": {},
   "outputs": [],
   "source": [
    "asin_key_check = 60526157\n",
    "df_sentiment_check = df_sentiment.select(\"reviewText\",\"overall\",\"sentiment\") \\\n",
    "   .filter(f\"asin_key == {asin_key_check}\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2e046e7a-b40b-4bb8-a673-9e5a4c4c11e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer indicated score: 1.0 | Sentiment: negative\n",
      "======================\n",
      "If you enjoyed where the Red Fern Grows this is the book for you.  Sadist.\n"
     ]
    }
   ],
   "source": [
    "checktext = df_sentiment_check[0][\"reviewText\"]\n",
    "checkoverall = df_sentiment_check[0][\"overall\"]\n",
    "checksentiment = df_sentiment_check[0][\"sentiment\"]\n",
    "print(f\"Customer indicated score: {checkoverall} | Sentiment: {checksentiment}\")\n",
    "print(\"======================\")\n",
    "print(checktext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c278217-3e8f-4d06-8b2a-b2123988ae2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
