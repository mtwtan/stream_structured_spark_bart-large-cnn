{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe5a8a0e-1189-47d3-877b-1093328563ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import HiveContext, SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from delta import *\n",
    "import boto3\n",
    "import json\n",
    "import math\n",
    "#import os\n",
    "#import pandas as pd\n",
    "#import numpy as np\n",
    "#import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e91a7e53-6c51-47e0-b1a1-bdd5fb9076c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Variables\n",
    "BOOTSTRAP_SERVERS = \"boot-XXXXXXXX.c1.kafka-serverless.us-east-1.amazonaws.com:9098\"\n",
    "REGION = \"us-east-1\"\n",
    "TOPIC = \"amznbookreviews\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0430dfc9-85a1-4929-a89f-b22ac4156a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark-3.5.0-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ubuntu/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ubuntu/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-de2bc315-6eb2-4232-ad99-3d46d34dbe8a;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.12;3.1.0 in local-m2-cache\n",
      "\tfound io.delta#delta-storage;3.1.0 in local-m2-cache\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in local-m2-cache\n",
      ":: resolution report :: resolve 141ms :: artifacts dl 7ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.12;3.1.0 from local-m2-cache in [default]\n",
      "\tio.delta#delta-storage;3.1.0 from local-m2-cache in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from local-m2-cache in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-de2bc315-6eb2-4232-ad99-3d46d34dbe8a\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/4ms)\n",
      "24/03/12 15:18:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "builder = SparkSession.builder.appName(\"amzn-reviews\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.driver.cores\", \"2\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\",\"28g\") \\\n",
    "    .config(\"spark.executor.cores\",\"5\") \\\n",
    "    .config(\"spark.executor.instances\",\"2\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\",True) \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", True) \\\n",
    "    .config(\"spark.sql.parquet.mergeSchema\", False) \\\n",
    "    .config(\"spark.hadoop.parquet.enable.summary-metadata\", False) \\\n",
    "    .enableHiveSupport()\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7c565ae-dc3e-445e-ada3-ee29987b4759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get secrets credential for S3a\n",
    "client = boto3.client('secretsmanager',region_name=REGION)\n",
    "response = client.get_secret_value(\n",
    "    SecretId='s3all'\n",
    ")\n",
    "accessJson = json.loads(response['SecretString'])\n",
    "accessKeyId = accessJson['accessKey']\n",
    "secretAccessKey = accessJson['secretAccess']\n",
    "\n",
    "# Configure S3a\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", accessKeyId)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", secretAccessKey)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.us-east-1.amazonaws.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7c872e-ecdc-4049-9840-9b47224483cc",
   "metadata": {},
   "source": [
    "# ForEachBatch definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a489c7ec-5b70-46fc-9337-d61acd696624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preBatchRecords(microBatchDf, batchSize):\n",
    "    batch_count = math.ceil(microBatchDf.count() / batchSize)\n",
    "    # % sign is modulus -- remainder after division\n",
    "    microBatchDf = microBatchDf.withColumn(\"batch_id\", col(\"row_number\") % batch_count)\n",
    "    microBatchDf = microBatchDf.withColumn(\"bartUpdated\", lit(\"N\"))\n",
    "    microBatchDf = microBatchDf.withColumn(\"bartSummary\", lit(\"\"))\n",
    "\n",
    "    return microBatchDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc9b7e02-f68f-46c9-a8b5-5536a8470b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def callUdfBatch(df, batchId):\n",
    "    path = \"s3a://amzn-customer-reviews-XXXXXXXXXXXX/sink/test/test-streaming-foreach-pandas/\"\n",
    "    submitted_df = preBatchRecords(df, 5)\n",
    "\n",
    "    submitted_df.write.format(\"delta\").mode(\"append\").save(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d633a7-c13f-441c-9231-dc14b9e03c7b",
   "metadata": {},
   "source": [
    "# Reading from Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84b8c4fc-ea1d-4347-955b-cd0ebfcccba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting the Kafka options\n",
    "options_read = {\n",
    "    \"kafka.bootstrap.servers\": BOOTSTRAP_SERVERS,\n",
    "    \"subscribe\": TOPIC,\n",
    "    \"startingOffsets\": \"latest\",\n",
    "    \"kafka.security.protocol\": \"SASL_SSL\",\n",
    "    \"kafka.sasl.mechanism\": \"AWS_MSK_IAM\",\n",
    "    \"kafka.sasl.jaas.config\": \"software.amazon.msk.auth.iam.IAMLoginModule required;\",\n",
    "    \"kafka.sasl.client.callback.handler.class\": \"software.amazon.msk.auth.iam.IAMClientCallbackHandler\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "330fb62e-8b59-4be5-b634-c558dcc993fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read from Kafka\n",
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .options(**options_read) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa97ae5-8b18-41e4-8495-8f7a3c10a3c7",
   "metadata": {},
   "source": [
    "# Writing to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dab08fb0-ed3d-497e-a3db-93b5d43fcbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting the Checkpoint location\n",
    "checkpoint_path = \"s3a://amzn-customer-reviews-XXXXXXXXXXX/checkpoint/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07aa16f3-1bba-4574-b48c-9a7f2381260d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting schema of Kafka message topic\n",
    "json_schema = StructType([\n",
    "    StructField('asin', StringType(), True),\n",
    "    StructField('overall', DoubleType(), True),\n",
    "    StructField('reviewText', StringType(), True),\n",
    "    StructField('reviewTimeTS', TimestampType(), True),\n",
    "    StructField('reviewerID', StringType(), True),\n",
    "    StructField('reviewerName', StringType(), True),\n",
    "    StructField('summary', StringType(), True),\n",
    "    StructField('verified', BooleanType(), True),\n",
    "    StructField('row_number', IntegerType(), True),\n",
    "    StructField('asin_key', IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09c14686-5d32-45af-85fa-f25f67cd2228",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/12 15:18:52 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-c8bfaad4-5597-4919-9d73-60f75d2c86de. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/03/12 15:18:52 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/03/12 15:18:55 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "24/03/12 15:18:57 WARN ParquetOutputFormat: Setting parquet.enable.summary-metadata is deprecated, please use parquet.summary.metadata.level\n",
      "24/03/12 15:19:00 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "24/03/12 15:19:21 WARN ParquetOutputFormat: Setting parquet.enable.summary-metadata is deprecated, please use parquet.summary.metadata.level\n",
      "24/03/12 15:19:40 WARN ParquetOutputFormat: Setting parquet.enable.summary-metadata is deprecated, please use parquet.summary.metadata.level\n",
      "24/03/12 15:20:00 WARN ParquetOutputFormat: Setting parquet.enable.summary-metadata is deprecated, please use parquet.summary.metadata.level\n",
      "ERROR:root:KeyboardInterrupt while sending command.                             \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m streamHandle \u001b[38;5;241m=\u001b[39m (df\n\u001b[1;32m      2\u001b[0m                 \u001b[38;5;241m.\u001b[39mselectExpr(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCAST(key as STRING)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCAST(value as STRING)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m                 \u001b[38;5;241m.\u001b[39mselect(from_json(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m,json_schema)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata.*\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m                 \u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m      8\u001b[0m                )\n\u001b[0;32m---> 10\u001b[0m \u001b[43mstreamHandle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/streaming/query.py:221\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "streamHandle = (df\n",
    "                .selectExpr(\"CAST(key as STRING)\",\"CAST(value as STRING)\")\n",
    "                .select(from_json(\"value\",json_schema).alias(\"data\")).select(\"data.*\")\n",
    "                .writeStream\n",
    "                .foreachBatch(callUdfBatch)\n",
    "                .trigger(processingTime='20 seconds')\n",
    "                .start()\n",
    "               )\n",
    "\n",
    "streamHandle.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c11e2e-f615-4ee7-ba87-a92128e76c39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
